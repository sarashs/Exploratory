{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d56965-33d1-48f9-b679-fd91b9c1949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.10).\n",
      "Path to dataset files: /home/ubuntu/.cache/kagglehub/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"columbine/imdb-dataset-sentiment-analysis-in-csv-format\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bed025-98eb-472b-b1a2-23e282e137b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('Train.csv')  # or however you load\n",
    "df_val = pd.read_csv('Valid.csv')\n",
    "le = LabelEncoder()\n",
    "df_train[\"label_id\"] = le.fit_transform(df_train[\"label\"])  # e.g. cat=0, dog=1, car=2, etc.\n",
    "df_val[\"label_id\"] = le.fit_transform(df_val[\"label\"]) \n",
    "num_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45a5018-7250-4aa8-8947-387663732480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60a82d1-db41-4030-ad1a-6fad20f7beb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c786830-f552-42eb-97c4-ac0ca2a70a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=32):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label_id\"].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # encoding[\"input_ids\"]: shape [1, max_len]\n",
    "        # We want them as 1D tensors, so we do .squeeze(0)\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c97ccf9-f125-4dde-8c7c-cfd0b6ddad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_contrastive_loss(embeddings, labels, temperature=0.07):\n",
    "    \"\"\"\n",
    "    embeddings: Tensor [batch_size, embed_dim]\n",
    "    labels:     Tensor [batch_size]\n",
    "    \"\"\"\n",
    "    # Normalize for cosine similarity\n",
    "    emb = nn.functional.normalize(embeddings, p=2, dim=1)  # [B, D]\n",
    "    batch_size = emb.size(0)\n",
    "\n",
    "    # Compute cosine similarity between all pairs => [B, B]\n",
    "    sim_matrix = torch.matmul(emb, emb.t())  # shape [B, B]\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "\n",
    "    # Mask out self-comparisons on diagonal\n",
    "    logits_mask = torch.ones_like(sim_matrix) - torch.eye(batch_size, device=sim_matrix.device)\n",
    "    sim_matrix = sim_matrix * logits_mask\n",
    "\n",
    "    # Build positives mask (same label => 1, else 0), then remove self\n",
    "    labels = labels.unsqueeze(1)  # [B,1]\n",
    "    match_mask = (labels == labels.t()).float()  # [B,B]\n",
    "    positives_mask = match_mask * logits_mask\n",
    "\n",
    "    # Exponentiate similarity\n",
    "    exp_sim = torch.exp(sim_matrix)\n",
    "    # Numerator = sum of exp(sim) over positives\n",
    "    numerator = torch.sum(exp_sim * positives_mask, dim=1)\n",
    "    # Denominator = sum of exp(sim) over entire row\n",
    "    denominator = torch.sum(exp_sim, dim=1)\n",
    "\n",
    "    loss = -torch.log(numerator / (denominator + 1e-12) + 1e-12)\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e181091-e806-47e1-a1f5-a2de44de7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForContrastive(nn.Module):\n",
    "    def __init__(self, pretrained_name=\"bert-base-uncased\", dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_name)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # AutoModel returns last_hidden_state, pooler_output, etc.\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # last_hidden_state => [batch_size, seq_len, hidden_dim]\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        # We'll do average pooling\n",
    "        pooled = last_hidden.mean(dim=1)  # [batch_size, hidden_dim]\n",
    "        embeddings = self.dropout(pooled)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "176595aa-f8a3-40b5-be95-08bf41e46ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive_pytorch(\n",
    "    df_train,\n",
    "    df_val,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device=\"cuda\",\n",
    "    epochs=5,\n",
    "    batch_size=8,\n",
    "    patience=2,\n",
    "    lr=2e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains `model` (instance of BertForContrastive or similar) on df_train with\n",
    "    supervised contrastive loss, using df_val for early stopping.\n",
    "\n",
    "    Args:\n",
    "      df_train: DataFrame with columns [\"text\", \"label_id\"] for training.\n",
    "      df_val:   DataFrame with columns [\"text\", \"label_id\"] for validation.\n",
    "      tokenizer: a Hugging Face tokenizer (e.g. AutoTokenizer.from_pretrained(...))\n",
    "      model:   an instance of BertForContrastive (nn.Module)\n",
    "      device:  \"cuda\" or \"cpu\"\n",
    "      epochs:  max number of epochs\n",
    "      batch_size: how many samples per batch\n",
    "      patience: how many epochs of no improvement before stopping\n",
    "      lr:      learning rate\n",
    "\n",
    "    Returns:\n",
    "      The best-scoring (lowest val loss) model (already loaded with best weights).\n",
    "    \"\"\"\n",
    "    # Build Dataset / DataLoader for train & val\n",
    "    train_dataset = TextDataset(df_train, tokenizer, max_len=128)\n",
    "    val_dataset   = TextDataset(df_val,   tokenizer, max_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # We'll save the best modelâ€™s state here\n",
    "    best_model_path = \"best_model.pth\"\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        ###################################################################\n",
    "        # 1) Training loop\n",
    "        ###################################################################\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = supervised_contrastive_loss(embeddings, labels, temperature=0.07)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        ###################################################################\n",
    "        # 2) Validation loop\n",
    "        ###################################################################\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                val_embeddings = model(input_ids, attention_mask=attention_mask)\n",
    "                val_loss = supervised_contrastive_loss(val_embeddings, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}] | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        ###################################################################\n",
    "        # 3) Early stopping check\n",
    "        ###################################################################\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model so far\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    ###################################################################\n",
    "    # 4) Load the best model before returning\n",
    "    ###################################################################\n",
    "    print(f\"Loading best model from val loss = {best_val_loss:.4f}\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a9d53c-e417-4da2-97a2-5ba17b2dfe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10] | Train Loss: 0.3510 | Val Loss: 0.3344\n",
      "Epoch [2/10] | Train Loss: 0.2403 | Val Loss: 0.3203\n",
      "Epoch [3/10] | Train Loss: 0.1490 | Val Loss: 0.3483\n",
      "Epoch [4/10] | Train Loss: 0.0797 | Val Loss: 0.3582\n",
      "Epoch [5/10] | Train Loss: 0.0439 | Val Loss: 0.3799\n",
      "Early stopping triggered.\n",
      "Loading best model from val loss = 0.3203\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = BertForContrastive(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "model = train_contrastive_pytorch(\n",
    "    df_train=df_train,\n",
    "    df_val=df_val,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    patience=3\n",
    "    lr=1e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f648c408-03af-4bdf-87d1-e1aeb0edbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "torch.save(model.state_dict(), \"distilbert_trained.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47f8c29a-bd9f-47fd-92a7-e72d381bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_inference(model_class, checkpoint_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    model_class: the class you used for training (e.g., BertForContrastive).\n",
    "    checkpoint_path: path to your saved model weights (e.g. \"best_model.pth\").\n",
    "    device: \"cuda\" if GPU is available, otherwise \"cpu\".\n",
    "    \"\"\"\n",
    "    model = model_class()\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  # set to inference mode (disables dropout, etc.)\n",
    "    return model\n",
    "\n",
    "def get_embeddings(texts, model, tokenizer, device=\"cuda\", max_length=32):\n",
    "    \"\"\"\n",
    "    Produce embeddings for one or multiple texts using a trained model.\n",
    "\n",
    "    Args:\n",
    "      texts: a single string or a list of strings\n",
    "      model: a trained nn.Module (e.g. BertForContrastive) in eval mode\n",
    "      tokenizer: the same tokenizer used in training (AutoTokenizer, etc.)\n",
    "      device: \"cuda\" or \"cpu\"\n",
    "      max_length: max tokens per text (adjust to match your training setup)\n",
    "\n",
    "    Returns:\n",
    "      A NumPy array of shape [batch_size, embedding_dim] with the embeddings.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]  # convert single string to list\n",
    "\n",
    "    # Tokenize the input\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass => shape [batch_size, embedding_dim]\n",
    "        embeddings = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Convert to CPU NumPy array for convenient use\n",
    "        embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "    return embeddings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beda95ed-e75d-4a5a-a091-afd0f363dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# 3) Load the model\n",
    "#    Make sure you import/define the BertForContrastive class somewhere\n",
    "#    from my_code import BertForContrastive\n",
    "model = load_model_for_inference(\n",
    "    model_class=lambda: BertForContrastive(pretrained_name=\"distilbert-base-uncased\"),\n",
    "    checkpoint_path=\"distilbert_trained.pth\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b09005c-2631-4957-9cb7-bfa897c0140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_encode = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"Dogs are great pets\",\n",
    "    \"Cars drive on roads\"\n",
    "]\n",
    "embeddings = get_embeddings(texts_to_encode, model, tokenizer, device=device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ee684e2-fcdc-485c-a876-270a74175e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fca40-8498-43e5-b5e2-09aa568c8d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
