## Motivation

- While agentic AI is increasingly replacing traditional task-based prompt engineering, certain NLP data processing tasks still benefit from classical Q&A prompting. These tasks typically share two features: (1) They are terminal, requiring only a single-step interaction rather than hierarchical or sequential execution; (2) The volume of data to process places practical limits on the number of allowed LLM interactions, making single-prompt solutions preferable. Examples include text classification, sentiment analysis, compliance monitoring of electronic communications, and control monitoring.
- Prompt engineering remains a challenging and labor-intensive process, with prior efforts attempting to automate it \[reference needed\]. Our work builds on these efforts by framing prompt engineering as a reinforcement learning policy optimization problem. Specifically, we employ an $\epsilon$-greedy strategy to optimize prompt selection, aiming to maximize task-specific metric scores (expected rewards) on labeled datasets.

## Implementation
- we borrow ideas from reinforcement learning to optimize prompt selection. We implement an $\epsilon$-greedy strategy, where with probability $\epsilon$, we explore new prompts, and with probability $1-\epsilon$, we exploit the best-known prompt based on performance metrics. This approach allows us to balance exploration of new prompt variations with exploitation of effective prompts, ultimately aiming to maximize task-specific metric scores (expected rewards) on labeled datasets.
- In order to overcome the biases induced by the LLM, the initial prompt and the meta-prompts in action selection (e.g. constantly selecting the same action or completely ignoring some actions), we introduce a dropout mechanism. Specifically, we randomly drop out some actions with a certain probability during the action selection process. This encourages exploration of different prompts and helps to avoid overfitting to specific prompt patterns. Furthermore, we shuffle the order of actions to reduce positional bias, ensuring that the model does not favor prompts based on their position in the list.