{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Any, Literal, Callable, NamedTuple\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from prompt_engineer import *\n",
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde2d3e",
   "metadata": {},
   "source": [
    "## Experiment 1: IMDB Sentiment Analysis with Prompt Optimization\n",
    "\n",
    "### Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb998f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialPromptOutputFormat(BaseModel):\n",
    "    \"\"\"Output format for the initial prompt.\"\"\"\n",
    "    sentiment: bool = Field(..., description=\"sentiment analysis of the text either positive/True or negarive/False\")\n",
    "    other: str = Field(..., description=\"Everything else requested by the prompt in string format.\")\n",
    "\n",
    "initial_prompt=PromptTemplateData(\n",
    "    prompt=\"For the following given text return true if the sentiment is positive, otherwise return false. \\nText: {text}\",\n",
    "    system_prompt=\"You are an expert in extracting the comment sentiment from the given text.\",\n",
    "    output_format=InitialPromptOutputFormat,\n",
    "    prompt_format_function=text_format_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Train.csv\")\n",
    "\n",
    "if not os.path.exists(\"bad_data.csv\"):\n",
    "    # Function to process a single row for multiprocessing\n",
    "    def process_single_row(row_data):\n",
    "        \"\"\"Process a single row for LLM call - needs to be at module level for pickling\"\"\"\n",
    "        index, row = row_data\n",
    "        \n",
    "        # Create the prompt template for this specific row\n",
    "        prompt = PromptTemplateData(\n",
    "            prompt=\"For the following given text return true if the sentiment is positive, otherwise return false. \\nText: {text}\",\n",
    "            system_prompt=\"You are an expert in extracting the comment sentiment from the given text.\",\n",
    "            output_format=InitialPromptOutputFormat,\n",
    "            prompt_format_function=lambda x, y: x.format(text=y)\n",
    "        )\n",
    "        \n",
    "        # Format the prompt with the text\n",
    "        formatted_prompt = prompt(row['text'])\n",
    "        \n",
    "        # Make the LLM call\n",
    "        llm_response = llm_call(\n",
    "            prompt=formatted_prompt,\n",
    "            model=\"gpt-4o-mini\"\n",
    "        )\n",
    "        return index, row, llm_response\n",
    "\n",
    "    # creating a dataset where gpt-4o-mini performs poorly\n",
    "    bad_data = {'text': [], 'label': []}\n",
    "\n",
    "    # Prepare data for multiprocessing\n",
    "    rows_to_process = [(index, row) for index, row in train.iterrows()]\n",
    "\n",
    "    # Use multiprocessing with progress tracking\n",
    "    max_workers = min(mp.cpu_count(), 8)  # Adjust based on API rate limits\n",
    "    batch_size = 50  # Process in batches to avoid overwhelming the API\n",
    "\n",
    "    print(f\"Using {max_workers} workers for parallel processing...\")\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit batches of work\n",
    "        futures = []\n",
    "        for i in range(0, len(rows_to_process), batch_size):\n",
    "            batch = rows_to_process[i:i + batch_size]\n",
    "            batch_futures = [executor.submit(process_single_row, row_data) for row_data in batch]\n",
    "            futures.extend(batch_futures)\n",
    "            \n",
    "            # Process completed futures from this batch\n",
    "            for future in as_completed(batch_futures):\n",
    "                try:\n",
    "                    index, row, llm_response = future.result()\n",
    "                    if llm_response.sentiment != (row['label'] == 1):\n",
    "                        bad_data['text'].append(row['text'])\n",
    "                        bad_data['label'].append(row['label'])\n",
    "                    \n",
    "                    # Break early if we have enough bad examples\n",
    "                    if len(bad_data['text']) >= 100:\n",
    "                        # Cancel remaining futures\n",
    "                        for f in futures:\n",
    "                            f.cancel()\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {index}: {e}\")\n",
    "            \n",
    "            # Break if we have enough data\n",
    "            if len(bad_data['text']) >= 100:\n",
    "                break\n",
    "            \n",
    "            print(f\"Processed batch {i//batch_size + 1}, found {len(bad_data['text'])} bad examples so far...\")\n",
    "\n",
    "    training_dataset = pd.DataFrame(bad_data)\n",
    "    print(f\"Created training dataset with {len(training_dataset)} examples\")\n",
    "    training_dataset.to_csv(\"bad_data.csv\", index=False)\n",
    "else:\n",
    "    training_dataset = pd.read_csv(\"bad_data.csv\")\n",
    "    print(f\"Loaded existing training dataset with {len(training_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4c7db",
   "metadata": {},
   "source": [
    "### Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_single_evaluation(args):\n",
    "    \"\"\"Process a single row for evaluation - needs to be at module level for pickling\"\"\"\n",
    "    prompt, row_data, model = args\n",
    "    index, row = row_data\n",
    "    \n",
    "    formatted_prompt = prompt(row['text'])\n",
    "    llm_response = llm_call(formatted_prompt,temperature=0, model=model)\n",
    "    prediction = 1 if llm_response.sentiment else 0\n",
    "    \n",
    "    return index, prediction, row['label'], row['text']\n",
    "\n",
    "def evaluation_method(prompt: PromptTemplateData, dataset: pd.DataFrame) -> Dict[str, float]:\n",
    "    def convert_to_sentiment(x):\n",
    "        return \"positive\" if x == 1 else \"negative\"\n",
    "    # Prepare data for multiprocessing\n",
    "    rows_to_process = [(index, row) for index, row in dataset.iterrows()]\n",
    "    \n",
    "    # Create arguments for each process\n",
    "    process_args = [(prompt, row_data, \"gpt-4o-mini\") for row_data in rows_to_process]\n",
    "    \n",
    "    # Use multiprocessing with progress tracking\n",
    "    max_workers = min(mp.cpu_count(), 8)  # Adjust based on API rate limits\n",
    "    \n",
    "    predictions = []\n",
    "    actual = []\n",
    "    failure_cases = []\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        futures = [executor.submit(process_single_evaluation, args) for args in process_args]\n",
    "        \n",
    "        # Collect results with progress bar\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                index, prediction, label, text = future.result()\n",
    "                predictions.append(prediction)\n",
    "                actual.append(label)\n",
    "                \n",
    "                # Check for failure cases\n",
    "                if prediction != label:\n",
    "                    failure_cases.append(f\"text: {text}| predicted: {convert_to_sentiment(prediction)}| actual: {convert_to_sentiment(label)}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                # Handle failed cases by adding default values\n",
    "                predictions.append(0)\n",
    "                actual.append(0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = sum(p == a == 1 for p, a in zip(predictions, actual)) / sum(p == 1 for p in predictions) if sum(p == 1 for p in predictions) > 0 else 0\n",
    "    recall = sum(p == a == 1 for p, a in zip(predictions, actual)) / sum(a == 1 for a in actual) if sum(a == 1 for a in actual) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": sum(p == a for p, a in zip(predictions, actual)) / len(actual),\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1_score\": 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0,\n",
    "        \"false_positive_rate\": sum(p == 1 and a == 0 for p, a in zip(predictions, actual)) / sum(a == 0 for a in actual) if sum(a == 0 for a in actual) > 0 else 0,\n",
    "        \"failure_cases\": failure_cases\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffa9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 100\n",
    "TARGET_SCORE = 0.99\n",
    "EPSILON = 0.5  # exploration rate\n",
    "DROP_OUT = 0.5 # dropout rate for improvement actions\n",
    "for EPSILON in notebook.tqdm([0.1, 0.3, 0.5]):\n",
    "    for DROP_OUT in [0.1, 0.3, 0.5]:\n",
    "        print(f\"Running with EPSILON={EPSILON}, DROP_OUT={DROP_OUT}, MAX_ITERATIONS={MAX_ITERATIONS}\")\n",
    "        po = PromptOptimizer(\n",
    "            evaluation_method=evaluation_method,     # evaluation method for demonstration\n",
    "            training_dataset=training_dataset,\n",
    "            action_selection=action_selection_prompt,\n",
    "            action_application=action_application,\n",
    "            initial_prompt=initial_prompt,\n",
    "            action_list=IMPROVEMENT_ACTIONS,\n",
    "            train_test_ratio=0.5,\n",
    "            max_iterations=MAX_ITERATIONS,\n",
    "            target_score=TARGET_SCORE,\n",
    "            epsilon=EPSILON,\n",
    "            drop_out=DROP_OUT,\n",
    "        )\n",
    "        final_state = po.run()\n",
    "        save_final_state(final_state, EPSILON, DROP_OUT, MAX_ITERATIONS)\n",
    "\n",
    "#po.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_f1_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadadd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_max_f1_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
