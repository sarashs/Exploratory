{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Any\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from collections import deque\n",
    "\n",
    "# PARAMETERS\n",
    "DEBUG = True\n",
    "#--------------------------------------------------------------------------------\n",
    "# Structure for the outputs\n",
    "\n",
    "class Status(BaseModel):\n",
    "    status: str = Field(description=\"The status of the chain of thoughts. Either we should continue to reach a final asnwer, we should terminate this chain because it won't reach a final answer and it is going stray. If a we are ready for a final answer (regardless of correctness) return ready\", enum=[\"continue\", \"terminate\", \"ready\"])\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    \"\"\"A thought on how to solve the problem\"\"\"\n",
    "    thought: str = Field(description=\"The thought for the next step\")\n",
    "\n",
    "class Solution(BaseModel):\n",
    "    \"\"\"A solution to solve the problem\"\"\"\n",
    "    solution: str = Field(description=\"The solution to the problem\")\n",
    "\n",
    "# OpenAI models low T, high T, and evaluator\n",
    "class OpenAIParse(object):\n",
    "\n",
    "    def __init__(self, model, response_format, system_prompt):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.response_format= response_format\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def __call__(self, prompt, temperature = 0):     \n",
    "        completion = self.client.beta.chat.completions.parse(\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format=self.response_format,\n",
    "            #logprobs=True,\n",
    "            #top_logprobs=3,\n",
    "        )\n",
    "        return completion.choices[0].message.parsed\n",
    "\n",
    "Thought_system_prompt = \"\"\"Given the user query and the chain of thoughts, generate the next step (a thought) to solve the problem but do not generate a solution.\n",
    "Also remember that you do not have access to any outside tools or sources of knowledge.\"\"\"\n",
    "Thought_user_prompt = \"User query: {query}\\nChain of thoughts: {chain_of_thoughts}\"\n",
    "Thought_model = OpenAIParse(\"gpt-4o-mini\", Thought, Thought_system_prompt)\n",
    "#--------------------------------------------------------------------------------\n",
    "Status_system_prompt = \"\"\"\n",
    "Given the user query and the chain of thoughts, evaluate the chain of thoughts and determine the status of the chain of thoughts.\n",
    "The chain of thoughts is a series of thoughts that are generated to solve a problem.\n",
    "The chain of thoughts can be in one of three states: continue, terminate, ready.\"\"\"\n",
    "Status_user_prompt = \"User query: {query}\\nChain of thoughts: {chain_of_thoughts}\"\n",
    "Status_model = OpenAIParse(\"gpt-4o-mini\", Status, Status_system_prompt)\n",
    "#--------------------------------------------------------------------------------\n",
    "Solution_system_prompt = \"\"\"\n",
    "Given the user query and the chain of thoughts, generate the solution to the problem.\n",
    "The solution is the final answer to the problem.\"\"\"\n",
    "Solution_user_prompt = \"User query: {query}\\nChain of thoughts: {chain_of_thoughts}\"\n",
    "Solution_model = OpenAIParse(\"gpt-4o-mini\", Solution, Solution_system_prompt)\n",
    "#--------------------------------------------------------------------------------\n",
    "Final_system_prompt = \"\"\"\n",
    "Given the user query and the set of solutions, evaluate the solutions and determine the best solution.\n",
    "Note that they may be multiple similar solutions.\"\"\"\n",
    "Final_user_prompt = \"User query: {query}\\nSolutions: {solutions}\"\n",
    "Final_model = OpenAIParse(\"gpt-4o-mini\", Solution, Final_system_prompt)\n",
    "\n",
    "# Binary tree of thoughts\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, thought, parent = None, status: Status = None):\n",
    "        self.thought = thought\n",
    "        self.thoughts = [thought]\n",
    "        self.status = status\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.depth = 0\n",
    "        if parent:\n",
    "            self.depth = parent.depth + 1\n",
    "            self.parent = parent\n",
    "            self.thoughts = parent.thoughts + [thought]\n",
    "\n",
    "class BinNode(Node):\n",
    "    def __init__(self, thought, parent = None, status: Status = None, low_temp = None, high_temp = None):\n",
    "        super().__init__(thought, parent, status)\n",
    "        self.low_temp = low_temp\n",
    "        self.high_temp = high_temp\n",
    "\n",
    "class Tree(object):\n",
    "    def __init__(self, status_model, thought_model, solution_model, max_depth: int = 5, max_children: int = 2):\n",
    "        self.root = None\n",
    "        self.leaves = deque([self.root]) # list of leaves\n",
    "        self.status_model = status_model\n",
    "        self.solution_nodes = []\n",
    "        self.solutions = []\n",
    "        self.max_depth = max_depth\n",
    "        self.max_children = max_children\n",
    "        self.thought_model = thought_model\n",
    "        self.solution_model = solution_model\n",
    "\n",
    "    def set_status(self, query):\n",
    "        for node in self.leaves:\n",
    "            if not node.status:\n",
    "                node.status = self.status_model(\n",
    "                    Status_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "                    )\n",
    "\n",
    "    def prune(self):\n",
    "        removal = []\n",
    "        for node in self.leaves:\n",
    "            if node.status.status == \"terminate\":\n",
    "                removal.append(node)\n",
    "            elif node.status.status == \"ready\":\n",
    "                self.solution_nodes.append(node)\n",
    "                removal.append(node)\n",
    "        for node in removal:\n",
    "            self.leaves.remove(node)\n",
    "\n",
    "    def __call__(self, query, T=0):\n",
    "        self.root = Node(thought = query, status = Status(status=\"continue\"))\n",
    "        node = self.root\n",
    "        self.leaves = deque([self.root])\n",
    "        for depth in range(self.max_depth):\n",
    "            while self.leaves and node.depth < depth:\n",
    "                if DEBUG:\n",
    "                    print(f\"Current Depth: {node.depth} out of {self.max_depth}\")\n",
    "                node = self.leaves.popleft()\n",
    "                for i in range(self.max_children):\n",
    "                    thought = self.thought_model(\n",
    "                        Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                        temperature = T)\n",
    "                    node.children.append(Node(thought = thought.thought, parent = node))\n",
    "                self.leaves.extend(node.children)\n",
    "            self.set_status(query)\n",
    "            self.prune()\n",
    "        self.solutions = [self.solution_model(\n",
    "            Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "            ).solution for node in self.solution_nodes]\n",
    "        if not self.solutions:\n",
    "            self.solutions = [\"No solution found\"]\n",
    "        return self.solutions\n",
    "\n",
    "class BinTree(Tree):\n",
    "    def __init__(self, status_model, thought_model, solution_model, max_depth: int = 5, low_temp = 0, high_temp = 1):\n",
    "        super().__init__(status_model=status_model, thought_model=thought_model, solution_model=solution_model, max_depth=max_depth)\n",
    "        self.root = None\n",
    "        self.leaves = deque([self.root]) # list of leaves\n",
    "        self.low_temp = low_temp\n",
    "        self.high_temp = high_temp\n",
    "\n",
    "    def __call__(self, query):\n",
    "        self.root = BinNode(thought = query, status = Status(status=\"continue\"))\n",
    "        node = self.root\n",
    "        self.leaves = deque([self.root])\n",
    "        for depth in range(self.max_depth):\n",
    "            while self.leaves and node.depth < self.max_depth:\n",
    "                if DEBUG:\n",
    "                    print(f\"Current Depth: {node.depth} out of {self.max_depth}\")\n",
    "                node = self.leaves.popleft()\n",
    "                thought = self.thought_model(\n",
    "                    Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                    temperature = self.low_temp)\n",
    "                node.low_temp = BinNode(thought = thought.thought, parent = node)\n",
    "                thought = self.thought_model(\n",
    "                    Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                    temperature = self.high_temp)\n",
    "                node.high_temp = BinNode(thought = thought.thought, parent = node)\n",
    "                self.leaves.extend([node.low_temp, node.high_temp])\n",
    "            self.set_status(query)\n",
    "            self.prune()\n",
    "        self.solutions = [self.solution_model(\n",
    "            Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "            ).solution for node in self.solution_nodes]\n",
    "        if not self.solutions:\n",
    "            self.solutions = [\"No solution found\"]\n",
    "        return self.solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Depth: 0 out of 5\n",
      "Current Depth: 0 out of 5\n",
      "Current Depth: 1 out of 5\n",
      "Current Depth: 1 out of 5\n",
      "Current Depth: 2 out of 5\n",
      "Current Depth: 2 out of 5\n",
      "Current Depth: 2 out of 5\n",
      "Current Depth: 2 out of 5\n",
      "Current Depth: 3 out of 5\n",
      "Current Depth: 3 out of 5\n",
      "Current Depth: 3 out of 5\n",
      "Current Depth: 3 out of 5\n",
      "Current Depth: 3 out of 5\n",
      "Current Depth: 3 out of 5\n",
      "Current Depth: 3 out of 5\n",
      "Current Depth: 3 out of 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x = 2 or x = -2', 'x = 2 or x = -2']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = BinTree(Status_model, Thought_model, Solution_model)\n",
    "tree(\"What is the solution to the equation x^2 - 4 = 0?\", T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.leaves[0].depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tree.leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Given the user query and the set of thoughts, generate the next step (a thought) to solve the problem but do not generate a solution. Also remember that you do not have access to any outside tools or sources of knowledge.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Query: What is the largest city in the united states? \\n Thoughts: \\n\"},\n",
    "    ],\n",
    "    response_format=Thought,\n",
    "    logprobs=True,\n",
    "    top_logprobs=3,\n",
    ")\n",
    "\n",
    "thought = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionTokenLogprob(token='{\"', bytes=[123, 34], logprob=0.0, top_logprobs=[TopLogprob(token='{\"', bytes=[123, 34], logprob=0.0), TopLogprob(token='{', bytes=[123], logprob=-18.25), TopLogprob(token=' {\"', bytes=[32, 123, 34], logprob=-22.125)]),\n",
       " ChatCompletionTokenLogprob(token='thought', bytes=[116, 104, 111, 117, 103, 104, 116], logprob=0.0, top_logprobs=[TopLogprob(token='thought', bytes=[116, 104, 111, 117, 103, 104, 116], logprob=0.0), TopLogprob(token='th', bytes=[116, 104], logprob=-20.0), TopLogprob(token='though', bytes=[116, 104, 111, 117, 103, 104], logprob=-24.0)]),\n",
       " ChatCompletionTokenLogprob(token='\":\"', bytes=[34, 58, 34], logprob=-1.9361265e-07, top_logprobs=[TopLogprob(token='\":\"', bytes=[34, 58, 34], logprob=-1.9361265e-07), TopLogprob(token='\":\"\\'', bytes=[34, 58, 34, 39], logprob=-16.5), TopLogprob(token='\":', bytes=[34, 58], logprob=-18.25)]),\n",
       " ChatCompletionTokenLogprob(token='I', bytes=[73], logprob=-0.2783952, top_logprobs=[TopLogprob(token='I', bytes=[73], logprob=-0.2783952), TopLogprob(token='Consider', bytes=[67, 111, 110, 115, 105, 100, 101, 114], logprob=-1.7783952), TopLogprob(token='To', bytes=[84, 111], logprob=-3.2783952)]),\n",
       " ChatCompletionTokenLogprob(token=' should', bytes=[32, 115, 104, 111, 117, 108, 100], logprob=-0.57711124, top_logprobs=[TopLogprob(token=' should', bytes=[32, 115, 104, 111, 117, 108, 100], logprob=-0.57711124), TopLogprob(token=' need', bytes=[32, 110, 101, 101, 100], logprob=-0.82711124), TopLogprob(token=' will', bytes=[32, 119, 105, 108, 108], logprob=-7.3271112)]),\n",
       " ChatCompletionTokenLogprob(token=' consider', bytes=[32, 99, 111, 110, 115, 105, 100, 101, 114], logprob=-0.27340162, top_logprobs=[TopLogprob(token=' consider', bytes=[32, 99, 111, 110, 115, 105, 100, 101, 114], logprob=-0.27340162), TopLogprob(token=' clarify', bytes=[32, 99, 108, 97, 114, 105, 102, 121], logprob=-2.8984017), TopLogprob(token=' determine', bytes=[32, 100, 101, 116, 101, 114, 109, 105, 110, 101], logprob=-3.5234017)]),\n",
       " ChatCompletionTokenLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=-0.44379976, top_logprobs=[TopLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=-0.44379976), TopLogprob(token=' looking', bytes=[32, 108, 111, 111, 107, 105, 110, 103], logprob=-2.1937997), TopLogprob(token=' what', bytes=[32, 119, 104, 97, 116], logprob=-2.4437997)]),\n",
       " ChatCompletionTokenLogprob(token=' most', bytes=[32, 109, 111, 115, 116], logprob=-0.5136043, top_logprobs=[TopLogprob(token=' most', bytes=[32, 109, 111, 115, 116], logprob=-0.5136043), TopLogprob(token=' population', bytes=[32, 112, 111, 112, 117, 108, 97, 116, 105, 111, 110], logprob=-2.0136042), TopLogprob(token=' latest', bytes=[32, 108, 97, 116, 101, 115, 116], logprob=-2.2636042)]),\n",
       " ChatCompletionTokenLogprob(token=' recent', bytes=[32, 114, 101, 99, 101, 110, 116], logprob=-0.03763653, top_logprobs=[TopLogprob(token=' recent', bytes=[32, 114, 101, 99, 101, 110, 116], logprob=-0.03763653), TopLogprob(token=' current', bytes=[32, 99, 117, 114, 114, 101, 110, 116], logprob=-3.7876365), TopLogprob(token=' up', bytes=[32, 117, 112], logprob=-5.0376368)]),\n",
       " ChatCompletionTokenLogprob(token=' population', bytes=[32, 112, 111, 112, 117, 108, 97, 116, 105, 111, 110], logprob=-0.10208693, top_logprobs=[TopLogprob(token=' population', bytes=[32, 112, 111, 112, 117, 108, 97, 116, 105, 111, 110], logprob=-0.10208693), TopLogprob(token=' census', bytes=[32, 99, 101, 110, 115, 117, 115], logprob=-2.602087), TopLogprob(token=' data', bytes=[32, 100, 97, 116, 97], logprob=-3.977087)]),\n",
       " ChatCompletionTokenLogprob(token=' data', bytes=[32, 100, 97, 116, 97], logprob=-0.08238116, top_logprobs=[TopLogprob(token=' data', bytes=[32, 100, 97, 116, 97], logprob=-0.08238116), TopLogprob(token=' statistics', bytes=[32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 115], logprob=-2.7073812), TopLogprob(token=' estimates', bytes=[32, 101, 115, 116, 105, 109, 97, 116, 101, 115], logprob=-4.4573812)]),\n",
       " ChatCompletionTokenLogprob(token=' to', bytes=[32, 116, 111], logprob=-1.3084071, top_logprobs=[TopLogprob(token=' to', bytes=[32, 116, 111], logprob=-1.3084071), TopLogprob(token=' for', bytes=[32, 102, 111, 114], logprob=-1.3084071), TopLogprob(token=' or', bytes=[32, 111, 114], logprob=-1.5584071)]),\n",
       " ChatCompletionTokenLogprob(token=' determine', bytes=[32, 100, 101, 116, 101, 114, 109, 105, 110, 101], logprob=-0.21519795, top_logprobs=[TopLogprob(token=' determine', bytes=[32, 100, 101, 116, 101, 114, 109, 105, 110, 101], logprob=-0.21519795), TopLogprob(token=' identify', bytes=[32, 105, 100, 101, 110, 116, 105, 102, 121], logprob=-1.7151979), TopLogprob(token=' accurately', bytes=[32, 97, 99, 99, 117, 114, 97, 116, 101, 108, 121], logprob=-5.215198)]),\n",
       " ChatCompletionTokenLogprob(token=' which', bytes=[32, 119, 104, 105, 99, 104], logprob=-0.090785906, top_logprobs=[TopLogprob(token=' which', bytes=[32, 119, 104, 105, 99, 104], logprob=-0.090785906), TopLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=-2.465786), TopLogprob(token=' what', bytes=[32, 119, 104, 97, 116], logprob=-6.590786)]),\n",
       " ChatCompletionTokenLogprob(token=' city', bytes=[32, 99, 105, 116, 121], logprob=-0.005059656, top_logprobs=[TopLogprob(token=' city', bytes=[32, 99, 105, 116, 121], logprob=-0.005059656), TopLogprob(token=' U', bytes=[32, 85], logprob=-5.3800597), TopLogprob(token=' US', bytes=[32, 85, 83], logprob=-8.380059)]),\n",
       " ChatCompletionTokenLogprob(token=' has', bytes=[32, 104, 97, 115], logprob=-0.9088625, top_logprobs=[TopLogprob(token=' has', bytes=[32, 104, 97, 115], logprob=-0.9088625), TopLogprob(token=' currently', bytes=[32, 99, 117, 114, 114, 101, 110, 116, 108, 121], logprob=-1.2838625), TopLogprob(token=' holds', bytes=[32, 104, 111, 108, 100, 115], logprob=-1.7838625)]),\n",
       " ChatCompletionTokenLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=-0.00031317843, top_logprobs=[TopLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=-0.00031317843), TopLogprob(token=' consistently', bytes=[32, 99, 111, 110, 115, 105, 115, 116, 101, 110, 116, 108, 121], logprob=-8.500313), TopLogprob(token=' grown', bytes=[32, 103, 114, 111, 119, 110], logprob=-10.125313)]),\n",
       " ChatCompletionTokenLogprob(token=' largest', bytes=[32, 108, 97, 114, 103, 101, 115, 116], logprob=-0.57614654, top_logprobs=[TopLogprob(token=' largest', bytes=[32, 108, 97, 114, 103, 101, 115, 116], logprob=-0.57614654), TopLogprob(token=' highest', bytes=[32, 104, 105, 103, 104, 101, 115, 116], logprob=-0.82614654), TopLogprob(token=' greatest', bytes=[32, 103, 114, 101, 97, 116, 101, 115, 116], logprob=-9.076146)]),\n",
       " ChatCompletionTokenLogprob(token=' population', bytes=[32, 112, 111, 112, 117, 108, 97, 116, 105, 111, 110], logprob=-0.038479317, top_logprobs=[TopLogprob(token=' population', bytes=[32, 112, 111, 112, 117, 108, 97, 116, 105, 111, 110], logprob=-0.038479317), TopLogprob(token=' number', bytes=[32, 110, 117, 109, 98, 101, 114], logprob=-3.2884793), TopLogprob(token=' total', bytes=[32, 116, 111, 116, 97, 108], logprob=-9.413479)]),\n",
       " ChatCompletionTokenLogprob(token=' in', bytes=[32, 105, 110], logprob=-0.31645167, top_logprobs=[TopLogprob(token=' in', bytes=[32, 105, 110], logprob=-0.31645167), TopLogprob(token=',', bytes=[44], logprob=-1.9414517), TopLogprob(token='.\"', bytes=[46, 34], logprob=-3.1914515)]),\n",
       " ChatCompletionTokenLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=-0.00045426787, top_logprobs=[TopLogprob(token=' the', bytes=[32, 116, 104, 101], logprob=-0.00045426787), TopLogprob(token=' comparison', bytes=[32, 99, 111, 109, 112, 97, 114, 105, 115, 111, 110], logprob=-8.375454), TopLogprob(token=' terms', bytes=[32, 116, 101, 114, 109, 115], logprob=-8.750454)]),\n",
       " ChatCompletionTokenLogprob(token=' United', bytes=[32, 85, 110, 105, 116, 101, 100], logprob=-0.018362746, top_logprobs=[TopLogprob(token=' United', bytes=[32, 85, 110, 105, 116, 101, 100], logprob=-0.018362746), TopLogprob(token=' U', bytes=[32, 85], logprob=-4.1433625), TopLogprob(token=' US', bytes=[32, 85, 83], logprob=-6.1433625)]),\n",
       " ChatCompletionTokenLogprob(token=' States', bytes=[32, 83, 116, 97, 116, 101, 115], logprob=0.0, top_logprobs=[TopLogprob(token=' States', bytes=[32, 83, 116, 97, 116, 101, 115], logprob=0.0), TopLogprob(token=' State', bytes=[32, 83, 116, 97, 116, 101], logprob=-18.125), TopLogprob(token=' states', bytes=[32, 115, 116, 97, 116, 101, 115], logprob=-19.125)]),\n",
       " ChatCompletionTokenLogprob(token='.\"', bytes=[46, 34], logprob=-0.12963665, top_logprobs=[TopLogprob(token='.\"', bytes=[46, 34], logprob=-0.12963665), TopLogprob(token=',', bytes=[44], logprob=-2.6296368), TopLogprob(token='.', bytes=[46], logprob=-3.2546368)]),\n",
       " ChatCompletionTokenLogprob(token='}', bytes=[125], logprob=-4.3202e-07, top_logprobs=[TopLogprob(token='}', bytes=[125], logprob=-4.3202e-07), TopLogprob(token=' }', bytes=[32, 125], logprob=-15.0), TopLogprob(token='}\\n', bytes=[125, 10], logprob=-20.25)])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3][-1::-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
