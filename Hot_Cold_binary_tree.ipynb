{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# PARAMETERS\n",
    "DEBUG = True\n",
    "#--------------------------------------------------------------------------------\n",
    "# Structure for the outputs\n",
    "\n",
    "class Status(BaseModel):\n",
    "    status: str = Field(description=\"The status of the chain of thoughts. Either we should continue to reach a final asnwer, we should terminate this chain because it won't reach a final answer and it is going stray. If a we are ready for a final answer (regardless of correctness) return ready\", enum=[\"continue\", \"terminate\", \"ready\"])\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    \"\"\"A thought/step on how to solve the problem\"\"\"\n",
    "    thought: str = Field(description=\"The next step or thought to solve the problem\")\n",
    "\n",
    "class Solution(BaseModel):\n",
    "    \"\"\"A solution to the problem\"\"\"\n",
    "    solution: str = Field(description=\"The solution to the problem\")\n",
    "\n",
    "class SolutionEvaluation(BaseModel):\n",
    "    \"\"\"Evaluate and compare two solutions\"\"\"\n",
    "    reason: str = Field(description=\"Describe your reasoning for the evaluation\")\n",
    "    choice: str = Field(description=\"The best solution\", enum=[\"solution1\", \"solution2\"])\n",
    "\n",
    "class Equivalence(BaseModel):\n",
    "    \"\"\"Check if two solutions are equivalent\"\"\"\n",
    "    equivalent: bool = Field(description=\"True if the two solutions are equivalent, False otherwise\")\n",
    "\n",
    "# OpenAI models low T, high T, and evaluator\n",
    "class OpenAIParse(object):\n",
    "\n",
    "    def __init__(self, model, response_format, system_prompt):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.response_format= response_format\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def __call__(self, prompt, temperature = 0):     \n",
    "        completion = self.client.beta.chat.completions.parse(\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format=self.response_format,\n",
    "            #logprobs=True,\n",
    "            #top_logprobs=3,\n",
    "        )\n",
    "        return completion.choices[0].message.parsed\n",
    "#--------------------------------------------------------------------------------\n",
    "Thought_system_prompt = \"\"\"Given the user query and the chain of thoughts, generate the next step to solve the problem but do not generate the final solution.\n",
    "Do not repeat the same step twice. Your step/thought must follow the previous Chain of thoughts. If you are ready to generate a solution, return I am ready.\n",
    "You must use your own abilities and knowledge to generate the next step. Remember to generate only one step/thought at a time.\n",
    "Here is an example of a chain of thoughts:\n",
    "User query: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "Chain of thoughts:\n",
    "- Roger has 5 tennis balls.\n",
    "- 2 cans of 3 tennis balls each is 6 tennis balls.\n",
    "- 5 + 6 = 11 tennis balls.\n",
    "- I am ready.\n",
    "\"\"\"\n",
    "Thought_user_prompt = \"User query: {query}\\nChain of thoughts: {chain_of_thoughts}\"\n",
    "Thought_model = OpenAIParse(\"gpt-4o-mini\", Thought, Thought_system_prompt)\n",
    "#--------------------------------------------------------------------------------\n",
    "Status_system_prompt = \"\"\"\n",
    "Given the user query and the chain of thoughts, evaluate the chain of thoughts and determine the status of the chain of thoughts.\n",
    "The chain of thoughts is a series of thoughts that are generated to solve a problem.\n",
    "The chain of thoughts can be in one of three states: continue, terminate, ready.\"\"\"\n",
    "Status_user_prompt = \"User query: {query}\\nChain of thoughts: {chain_of_thoughts}\"\n",
    "Status_model = OpenAIParse(\"gpt-4o-mini\", Status, Status_system_prompt)\n",
    "#--------------------------------------------------------------------------------\n",
    "Solution_system_prompt = \"\"\"\n",
    "Given the user query and the chain of thoughts, generate the solution to the problem.\n",
    "The solution is the final answer to the problem.\"\"\"\n",
    "Solution_user_prompt = \"User query: {query}\\nChain of thoughts: {chain_of_thoughts}\"\n",
    "Solution_model = OpenAIParse(\"gpt-4o-mini\", Solution, Solution_system_prompt)\n",
    "#--------------------------------------------------------------------------------\n",
    "Equivalence_system_prompt = \"\"\"\n",
    "Given two solutions and a user query, evaluate if the two solutions are equivalent.\"\"\"\n",
    "Equivalence_user_prompt = \"User query: {query}\\nSolution 1: {solution_1}\\nSolution 2: {solution_2}\"\n",
    "Equivalence_model = OpenAIParse(\"gpt-4o-mini\", Equivalence, Equivalence_system_prompt)\n",
    "#--------------------------------------------------------------------------------\n",
    "Compare_system_prompt = \"\"\"\n",
    "Given the user query, chain of thought and solution 1 and, chain of thought and solution 2, evaluate the solutions and determine the best solution.\n",
    "Note that the solutions are already generated and you are evaluating them. You should consider the logical reasoning, logical flow, and correctness of the solutions.\"\"\"\n",
    "Compare_user_prompt = \"User query: {query}\\nSolutions 1: {solutions_1}\\nSolutions 2: {solutions_2}\"\n",
    "Compare_model = OpenAIParse(\"gpt-4o-mini\", SolutionEvaluation, Compare_system_prompt)\n",
    "#--------------------------------------------------------------------------------\n",
    "# chain of thoughts\n",
    "\n",
    "class link(object):\n",
    "    \"\"\"A link in a chain of thoughts\"\"\"\n",
    "    def __init__(self, thought, prev = None , status: Status = None):\n",
    "        self.thought = thought\n",
    "        self.thoughts = [thought]\n",
    "        self.status = status\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        if prev:\n",
    "            self.prev = prev\n",
    "            self.thoughts = prev.thoughts + [thought]\n",
    "\n",
    "class Chain(object):\n",
    "    \"\"\"A chain of thoughts to solve a problem\"\"\"\n",
    "    def __init__(self, status_model, thought_model, solution_model):\n",
    "        self.head = None\n",
    "        self.tail = None\n",
    "        self.status_model = status_model\n",
    "        self.thought_model = thought_model\n",
    "        self.solution_model = solution_model\n",
    "        self.solution = None\n",
    "\n",
    "    def set_status(self, node, query):\n",
    "        \"\"\"Set the status of the chain of thoughts\"\"\"\n",
    "        node.status = self.status_model(\n",
    "            Status_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "            )\n",
    "\n",
    "    def __call__(self, query, T = 0 , max_length: int = 5):\n",
    "        self.head = link(thought = query, status = Status(status=\"continue\"))\n",
    "        node = self.head\n",
    "        for i in range(max_length):\n",
    "            if DEBUG:\n",
    "                print(f\"Current Length: {i} out of {max_length}\")\n",
    "            thought = self.thought_model(\n",
    "                Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                temperature = T)\n",
    "            node.next = link(thought = thought.thought, prev = node)\n",
    "            node = node.next\n",
    "            self.set_status(node, query)\n",
    "            if node.status.status == \"terminate\" or node.status.status == \"ready\":\n",
    "                break\n",
    "        self.tail = node\n",
    "        self.solution = self.solution_model(\n",
    "            Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(self.tail.thoughts))\n",
    "            ).solution\n",
    "        return self.solution\n",
    "\n",
    "class AnnealChain(Chain):\n",
    "    \"\"\"A chain of thoughts to solve a problem using annealing\"\"\"\n",
    "    def __init__(self, status_model, thought_model, solution_model):\n",
    "        super().__init__(status_model, thought_model, solution_model)\n",
    "\n",
    "    def __call__(self, query, T_init = 1.5, max_length: int = 5):\n",
    "        \"\"\"Solve the problem using annealing\"\"\"\n",
    "        self.head = link(thought = query, status = Status(status=\"continue\"))\n",
    "        node = self.head\n",
    "        T_list = np.linspace(T_init, 0.0, max_length).tolist()\n",
    "        for T in T_list:\n",
    "            if DEBUG:\n",
    "                print(f\"Current temperature: {T} from {T_init}\")\n",
    "            thought = self.thought_model(\n",
    "                Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                temperature = T)\n",
    "            node.next = link(thought = thought.thought, prev = node)\n",
    "            node = node.next\n",
    "            self.set_status(node, query)\n",
    "            if node.status.status == \"terminate\" or node.status.status == \"ready\":\n",
    "                break\n",
    "        self.tail = node\n",
    "        self.solution = self.solution_model(\n",
    "            Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(self.tail.thoughts))\n",
    "            ).solution\n",
    "        return self.solution\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# tree of thoughts\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"A node in a tree of thoughts\"\"\"\n",
    "    def __init__(self, thought, parent = None, status: Status = None):\n",
    "        self.thought = thought\n",
    "        self.thoughts = [thought]\n",
    "        self.status = status\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.depth = 0\n",
    "        if parent:\n",
    "            self.depth = parent.depth + 1\n",
    "            self.parent = parent\n",
    "            self.thoughts = parent.thoughts + [thought]\n",
    "\n",
    "class BinNode(Node):\n",
    "    \"\"\"A node in a binary tree of thoughts\"\"\"\n",
    "    def __init__(self, thought, parent = None, status: Status = None, low_temp = None, high_temp = None, node_type = \"Hot\"):\n",
    "        super().__init__(thought, parent, status)\n",
    "        self.node_type_chain = [node_type] # list of node types to keep track of the path\n",
    "        self.low_temp = low_temp\n",
    "        self.high_temp = high_temp\n",
    "        if parent:\n",
    "            self.node_type_chain = parent.node_type_chain + [node_type]\n",
    "\n",
    "class Tree(object):\n",
    "    \"\"\"A tree of thoughts to solve a problem\"\"\"\n",
    "    def __init__(self, status_model, thought_model, solution_model,\n",
    "                 equivalence_model = None, best_solution_model = None, max_depth: int = 5, max_children: int = 2):\n",
    "        self.root = None\n",
    "        self.leaves = deque([self.root]) # list of leaves\n",
    "        self.status_model = status_model\n",
    "        self.solution_nodes = []\n",
    "        self.solutions = []\n",
    "        self.solution_thoughts = []\n",
    "        self.max_depth = max_depth\n",
    "        self.max_children = max_children\n",
    "        self.thought_model = thought_model\n",
    "        self.solution_model = solution_model\n",
    "        self.equivalence_model = equivalence_model\n",
    "        self.best_solution_model = best_solution_model\n",
    "\n",
    "    def set_status(self, query):\n",
    "        \"\"\"Set the status of the chain of thoughts\"\"\"\n",
    "        for node in self.leaves:\n",
    "            if not node.status:\n",
    "                node.status = self.status_model(\n",
    "                    Status_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "                    )\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\"Prune the tree by removing nodes that are ready to generate a solution or are terminated due to going stray.\"\"\"\n",
    "        removal = []\n",
    "        for node in self.leaves:\n",
    "            if node.status.status == \"terminate\":\n",
    "                removal.append(node)\n",
    "            elif node.status.status == \"ready\":\n",
    "                self.solution_nodes.append(node)\n",
    "                removal.append(node)\n",
    "        for node in removal:\n",
    "            self.leaves.remove(node)\n",
    "\n",
    "    def check_equivalence(self, query, solution_1, solution_2):\n",
    "        \"\"\"Check if two solutions are equivalent\"\"\"\n",
    "        return self.equivalence_model(\n",
    "            Equivalence_user_prompt.format(query=query, solution_1=solution_1, solution_2=solution_2)).equivalent\n",
    "    \n",
    "    def unique_solutions(self, query):\n",
    "        \"\"\"Remove duplicate solutions\"\"\"\n",
    "        unique_solutions = []\n",
    "        unique_solutions_thoughts = []\n",
    "        for num, solution in enumerate(self.solutions):\n",
    "            if not unique_solutions:\n",
    "                unique_solutions.append(solution)\n",
    "                unique_solutions_thoughts.append(self.solution_thoughts[num])\n",
    "            else:\n",
    "                for unique_solution in unique_solutions:\n",
    "                    if self.check_equivalence(query, solution, unique_solution):\n",
    "                        break\n",
    "                else:\n",
    "                    unique_solutions.append(solution)\n",
    "                    unique_solutions_thoughts.append(self.solution_thoughts[num])\n",
    "        self.solutions = unique_solutions\n",
    "        self.solution_thoughts = unique_solutions_thoughts\n",
    "\n",
    "    def compare_solution(self, query, solutions_1, solutions_2):\n",
    "        \"\"\"Evaluate and compare two solutions\"\"\"\n",
    "        return self.best_solution_model(\n",
    "            Compare_user_prompt.format(query=query, solutions_1=solutions_1, solutions_2=solutions_2)\n",
    "            ).choice\n",
    "    \n",
    "    def best_solution(self, query):\n",
    "        \"\"\"Find the best solution among the unique solutions\"\"\"\n",
    "        if len(self.solutions) == 1:\n",
    "            return (self.solution_thoughts[0], self.solutions[0]) if self.solution_thoughts else ([] , self.solutions[0])\n",
    "        else:\n",
    "            best = self.solutions[0]\n",
    "            best_thoughts = self.solution_thoughts[0]\n",
    "            for i in range(1, len(self.solutions)):\n",
    "                best_thoughts, best = (\n",
    "                    (self.solution_thoughts[i], self.solutions[i]) if self.compare_solution(\n",
    "                        query, best, '\\n-'.join(self.solution_thoughts[i])+\"\\n\"+self.solutions[i]\n",
    "                        ) == \"solution2\" else (best_thoughts, best)\n",
    "                        )\n",
    "            return (best_thoughts, best)\n",
    "\n",
    "    def __call__(self, query, T=0):\n",
    "        self.root = Node(thought = query, status = Status(status=\"continue\"))\n",
    "        node = self.root\n",
    "        self.leaves = deque([self.root])\n",
    "        for depth in range(self.max_depth):\n",
    "            while self.leaves and node.depth <= depth:\n",
    "                if DEBUG:\n",
    "                    print(f\"Current Depth: {node.depth} out of {self.max_depth}\")\n",
    "                node = self.leaves.popleft()\n",
    "                for i in range(self.max_children):\n",
    "                    thought = self.thought_model(\n",
    "                        Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                        temperature = T)\n",
    "                    node.children.append(Node(thought = thought.thought, parent = node))\n",
    "                self.leaves.extend(node.children)\n",
    "            self.set_status(query)\n",
    "            self.prune()\n",
    "        if self.solution_nodes:\n",
    "            self.solutions = [self.solution_model(\n",
    "                Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "                ).solution for node in self.solution_nodes]\n",
    "            self.solution_thoughts = [node.thoughts for node in self.solution_nodes]\n",
    "        elif self.leaves:\n",
    "            self.solutions = [self.solution_model(\n",
    "                Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "                ).solution for node in self.leaves]\n",
    "            self.solution_thoughts = [node.thoughts for node in self.leaves]\n",
    "        else:\n",
    "            self.solutions = [\"No solution found\"]\n",
    "        if self.equivalence_model:\n",
    "            self.unique_solutions(query)\n",
    "        if self.best_solution_model:\n",
    "            best_thoughts, best = self.best_solution(query)\n",
    "        if not best:\n",
    "            return None, self.solutions\n",
    "        return best_thoughts, best\n",
    "\n",
    "class BinTree(Tree):\n",
    "    \"\"\"A binary tree of thoughts to solve a problem\"\"\"\n",
    "    def __init__(\n",
    "            self, status_model, thought_model, solution_model,\n",
    "            equivalence_model = None, best_solution_model = None,\n",
    "            max_depth: int = 5, low_temp = 0, high_temp = 1.2\n",
    "            ):\n",
    "        super().__init__(\n",
    "            status_model=status_model, thought_model=thought_model,\n",
    "            solution_model=solution_model, equivalence_model=equivalence_model,\n",
    "            best_solution_model=best_solution_model, max_depth=max_depth\n",
    "            )\n",
    "        self.root = None\n",
    "        self.leaves = deque([self.root]) # list of leaves\n",
    "        self.low_temp = low_temp\n",
    "        self.high_temp = high_temp\n",
    "\n",
    "    def __call__(self, query):\n",
    "        self.root = BinNode(thought = query, status = Status(status=\"continue\"))\n",
    "        node = self.root\n",
    "        self.leaves = deque([self.root])\n",
    "        for depth in range(self.max_depth):\n",
    "            while self.leaves and node.depth <= self.max_depth:\n",
    "                if DEBUG:\n",
    "                    print(f\"Current Depth: {node.depth} out of {self.max_depth}\")\n",
    "                node = self.leaves.popleft()\n",
    "                thought = self.thought_model(\n",
    "                    Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                    temperature = self.low_temp)\n",
    "                node.low_temp = BinNode(thought = thought.thought, parent = node, node_type = \"Cold\")\n",
    "                thought = self.thought_model(\n",
    "                    Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                    temperature = self.high_temp)\n",
    "                node.high_temp = BinNode(thought = thought.thought, parent = node, node_type = \"Hot\")\n",
    "                self.leaves.extend([node.low_temp, node.high_temp])\n",
    "            self.set_status(query)\n",
    "            self.prune()\n",
    "        if self.solution_nodes:\n",
    "            self.solutions = [self.solution_model(\n",
    "                Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "                ).solution for node in self.solution_nodes]\n",
    "            self.solution_thoughts = [node.thoughts for node in self.solution_nodes]\n",
    "        elif self.leaves:\n",
    "            self.solutions = [self.solution_model(\n",
    "                Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "                ).solution for node in self.leaves]\n",
    "            self.solution_thoughts = [node.thoughts for node in self.leaves]\n",
    "        else:\n",
    "            self.solutions = [\"No solution found\"]        \n",
    "        if self.equivalence_model:\n",
    "            self.unique_solutions(query)\n",
    "        if self.best_solution_model:\n",
    "            best_thoughts, best = self.best_solution(query)\n",
    "        if not best:\n",
    "            return None, self.solutions\n",
    "        return best_thoughts, best\n",
    "    \n",
    "class AnnealTree(Tree):\n",
    "    \"\"\"A tree of thoughts to solve a problem using annealing\"\"\"\n",
    "    def __init__(self, status_model, thought_model, solution_model,\n",
    "                 equivalence_model = None, best_solution_model = None, max_depth: int = 5):\n",
    "        super().__init__(status_model, thought_model, solution_model, equivalence_model, best_solution_model, max_depth)\n",
    "\n",
    "    def __call__(self, query, T_init = 1.5):\n",
    "        \"\"\"Solve the problem using annealing\"\"\"\n",
    "        self.root = Node(thought = query, status = Status(status=\"continue\"))\n",
    "        node = self.root\n",
    "        self.leaves = deque([self.root])\n",
    "        temp_list = np.linspace(T_init, 0.0, self.max_depth).tolist()\n",
    "        for depth, T in zip(range(self.max_depth), temp_list):\n",
    "            while self.leaves and node.depth <= depth:\n",
    "                if DEBUG:\n",
    "                    print(f\"Current Depth: {node.depth} out of {self.max_depth}; Temperature: {T}\")\n",
    "                node = self.leaves.popleft()\n",
    "                for i in range(self.max_children):\n",
    "                    thought = self.thought_model(\n",
    "                        Thought_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts)), \n",
    "                        temperature = T)\n",
    "                    node.children.append(Node(thought = thought.thought, parent = node))\n",
    "                self.leaves.extend(node.children)\n",
    "            self.set_status(query)\n",
    "            self.prune()\n",
    "        if self.solution_nodes:\n",
    "            self.solutions = [self.solution_model(\n",
    "                Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "                ).solution for node in self.solution_nodes]\n",
    "            self.solution_thoughts = [node.thoughts for node in self.solution_nodes]\n",
    "        elif self.leaves:\n",
    "            self.solutions = [self.solution_model(\n",
    "                Solution_user_prompt.format(query=query, chain_of_thoughts='\\n-'.join(node.thoughts))\n",
    "                ).solution for node in self.leaves]\n",
    "            self.solution_thoughts = [node.thoughts for node in self.leaves]\n",
    "        else:\n",
    "            self.solutions = [\"No solution found\"]\n",
    "        if self.equivalence_model:\n",
    "            self.unique_solutions(query)\n",
    "        if self.best_solution_model:\n",
    "            best_thoughts, best = self.best_solution(query)\n",
    "        if not best:\n",
    "            return None, self.solutions\n",
    "        return best_thoughts, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Depth: 0 out of 5; Temperature: 1.5\n",
      "Current Depth: 0 out of 5; Temperature: 1.5\n",
      "Current Depth: 1 out of 5; Temperature: 1.125\n",
      "Current Depth: 1 out of 5; Temperature: 1.125\n",
      "Current Depth: 2 out of 5; Temperature: 0.75\n",
      "Current Depth: 2 out of 5; Temperature: 0.75\n",
      "Current Depth: 2 out of 5; Temperature: 0.75\n",
      "Current Depth: 2 out of 5; Temperature: 0.75\n",
      "Current Depth: 3 out of 5; Temperature: 0.375\n",
      "Current Depth: 3 out of 5; Temperature: 0.375\n",
      "Current Depth: 3 out of 5; Temperature: 0.375\n",
      "Current Depth: 4 out of 5; Temperature: 0.0\n",
      "Current Depth: 4 out of 5; Temperature: 0.0\n",
      "Current Depth: 4 out of 5; Temperature: 0.0\n",
      "Current Depth: 4 out of 5; Temperature: 0.0\n"
     ]
    }
   ],
   "source": [
    "#chain = AnnealChain(Status_model, Thought_model, Solution_model)\n",
    "#tree = BinTree(Status_model, Thought_model, Solution_model, Equivalence_model, Compare_model, max_depth=5)\n",
    "tree = AnnealTree(Status_model, Thought_model, Solution_model, Equivalence_model, Compare_model, max_depth=5)\n",
    "#tree(\"What is the solution to the equation x^2 - 4 = 0?\", T=1)\n",
    "#tree = Tree(Status_model, Thought_model, Solution_model, None, Compare_model, max_depth=5)\n",
    "thoughts, best = tree(\"If I am not not not not not hungry, do I want to eat?\", T_init=1.5)\n",
    "#chain(\"If I am not not not not not hungry, do I want to eat?\", T_init=1, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [1:39:29<05:14, 314.19s/it]\n"
     ]
    },
    {
     "ename": "LengthFinishReasonError",
     "evalue": "Could not parse response content as the length limit was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLengthFinishReasonError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m bin_tree_results\u001b[38;5;241m.\u001b[39mappend(best)\n\u001b[0;32m     19\u001b[0m tree \u001b[38;5;241m=\u001b[39m AnnealTree(Status_model, Thought_model, Solution_model, Equivalence_model, Compare_model, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m thoughts, best \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIf I am not not not not not hungry, do I want to eat?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m anneal_chain_results\u001b[38;5;241m.\u001b[39mappend(best)\n",
      "Cell \u001b[1;32mIn[1], line 390\u001b[0m, in \u001b[0;36mAnnealTree.__call__\u001b[1;34m(self, query, T_init)\u001b[0m\n\u001b[0;32m    388\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaves\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_children):\n\u001b[1;32m--> 390\u001b[0m     thought \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthought_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mThought_user_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_of_thoughts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthoughts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m     node\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend(Node(thought \u001b[38;5;241m=\u001b[39m thought\u001b[38;5;241m.\u001b[39mthought, parent \u001b[38;5;241m=\u001b[39m node))\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaves\u001b[38;5;241m.\u001b[39mextend(node\u001b[38;5;241m.\u001b[39mchildren)\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mOpenAIParse.__call__\u001b[1;34m(self, prompt, temperature)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt, temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):     \n\u001b[1;32m---> 42\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#logprobs=True,\u001b[39;49;00m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#top_logprobs=3,\u001b[39;49;00m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mparsed\n",
      "File \u001b[1;32mc:\\Users\\arash\\anaconda3\\lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py:140\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[1;34m(self, messages, model, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, seed, service_tier, stop, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    107\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Stainless-Helper-Method\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta.chat.completions.parse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    110\u001b[0m }\n\u001b[0;32m    112\u001b[0m raw_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    113\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    114\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    139\u001b[0m )\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arash\\anaconda3\\lib\\site-packages\\openai\\lib\\_parsing\\_completions.py:72\u001b[0m, in \u001b[0;36mparse_chat_completion\u001b[1;34m(response_format, input_tools, chat_completion)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m choice\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LengthFinishReasonError()\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m choice\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_filter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterFinishReasonError()\n",
      "\u001b[1;31mLengthFinishReasonError\u001b[0m: Could not parse response content as the length limit was reached"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "aneal_tree_results = []\n",
    "bin_tree_results = []\n",
    "chain_results = []\n",
    "anneal_chain_results = []\n",
    "for i in tqdm(range(20)):\n",
    "    chain = AnnealChain(Status_model, Thought_model, Solution_model)\n",
    "    try:\n",
    "        best = chain(\"If I am not not not not not hungry, do I want to eat?\", T_init=1.2, max_length=5)\n",
    "        anneal_chain_results.append(best)\n",
    "    except:\n",
    "        pass\n",
    "    chain = Chain(Status_model, Thought_model, Solution_model)\n",
    "    try:\n",
    "        best = chain(\"If I am not not not not not hungry, do I want to eat?\", max_length=5, T=1.2)\n",
    "        chain_results.append(best)\n",
    "    except:\n",
    "        pass\n",
    "    bintree = BinTree(Status_model, Thought_model, Solution_model, Equivalence_model, Compare_model, max_depth=5)\n",
    "    try:\n",
    "        thoughts, best = bintree(\"If I am not not not not not hungry, do I want to eat?\")\n",
    "        bin_tree_results.append(best)\n",
    "    except:\n",
    "        pass\n",
    "    tree = AnnealTree(Status_model, Thought_model, Solution_model, Equivalence_model, Compare_model, max_depth=5)\n",
    "    try:\n",
    "        thoughts, best = tree(\"If I am not not not not not hungry, do I want to eat?\", T_init=1.2)\n",
    "        aneal_tree_results.append(best)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes, I want to eat.',\n",
       " 'No, I do not want to eat.',\n",
       " 'Yes, I want to eat because I am hungry.',\n",
       " 'Yes, I want to eat.',\n",
       " 'Yes, you want to eat.',\n",
       " 'No, I do not want to eat.',\n",
       " 'No, you do not want to eat.',\n",
       " 'Yes, you want to eat.',\n",
       " 'Yes, if you are hungry, you want to eat.',\n",
       " 'Yes, you want to eat.',\n",
       " 'Yes, you want to eat.',\n",
       " 'No, I do not want to eat.',\n",
       " 'No, I do not want to eat.',\n",
       " 'No, I do not want to eat.',\n",
       " 'No, I do not want to eat.',\n",
       " 'No, I do not want to eat.',\n",
       " 'Yes, if you are not not not not not hungry, you are hungry, which typically means you want to eat.',\n",
       " 'No, you do not want to eat.',\n",
       " 'Yes, you want to eat.',\n",
       " 'Yes, you want to eat.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['If I am not not not not not hungry, do I want to eat?',\n",
       "  \"Determining whether 'not hungry' is an even number of hashes or an odd number helps ascertain my actual state of hunger after these qualifiers.\"],\n",
       " ['If I am not not not not not hungry, do I want to eat?',\n",
       "  \"To determine whether the repeated 'not' alters the meaning of hunger, I need to analyze the logical implications of each 'not'. Start by simplifying each notch of negation based on the rules of double negative application.\",\n",
       "  \"Each two 'not' phrases cancel each other out, appearing once the expressions are nested properly, meaning I should calculate the result step by step to carefully establish the real sentimentługiancers.\",\n",
       "  \"Start by simplifying the phrase 'not not not not not hungry' step by step, counting the number of 'not' phrases to see how many are left after applying the rules of negation.\",\n",
       "  \"Count the total number of 'not' phrases in the expression to determine if they result in an odd or even number, which will help in simplifying the statement correctly and understanding the final meaning of hunger in this context.\",\n",
       "  \"There are five 'not' phrases in total, so I need to determine if this results in an odd or even number of negations to simplify the statement about hunger correctly. Since five is odd, I will conclude that the final sentiment is 'not hungry'. Now I can assess whether this means I want to eat or not based on this conclusion.\"]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.solution_thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, you do not want to eat.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
